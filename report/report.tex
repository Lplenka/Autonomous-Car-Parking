\documentclass{svproc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm,algorithmic}% http://ctan.org/pkg/algorithms
\def\UrlFont{\rmfamily}

\newlength\myindent
\setlength\myindent{2em}
\newcommand\bindent{%
  \begingroup
  \setlength{\itemindent}{\myindent}
  \addtolength{\algorithmicindent}{\myindent}
}
\newcommand\eindent{\endgroup}

\begin{document}
\mainmatter
\title{Autonomous Car Parking
}
\subtitle{CS7IS2 Project (2021/2022)}
\author{Mayuresh Shelke, Ming Jun Lim, Lalu Prasad Lenka and Samridh James}

\institute{
\email{shelkem@tcd.ie}, \email{limm5@tcd.ie}, \email{lplenka@tcd.ie}, \email{jamessa@tcd.ie}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
%The abstract should summarize the contents of the report and should contain at least 70 and at most 150 words. 
%The abstract should give a concise overview of the main points of the report: the motivation behind the work, a very high level description of the problem and how it was solved by the proposed algorithms. The abstract must not include any figures or table.
Motivation behind the work, high level description of the problem, how it was solved by the proposed algorithms.
% TODO: Motivation behind the work
% TODO: High level description of the problem
% TODO: How it was solved by the proposed algorithms
\keywords{soft-actor-critic, behaviour cloning, evolutionary algorithm, parking}
\end{abstract}
%


%The report will be graded on the basis of:
%
%\begin{itemize}
%\item Originality - 10\%;
%\item Technical soundness - 20\%;
%\item Organisation - 20\%;
%\item Clarity of presentation - 20\%;
%\item Adequacy of bibliography/Results - 10\%
%\item Presentation slides and recording) - 20\%
%\end{itemize}


%\begin{description}
%Your report should provide a survey and an experimental comparison of multiple solution approaches to a particular problem. This is a critical review of at least three papers that significantly contributed to advance the state-of-the-art for the problem you are analysing. It should not be a mere summary of the papers. You are expected to conduct an analytical review of the methods under analysis to try to find common aspect and differences, connections between methods, drawbacks and open problems. Unless the faced problem has emerged recently, students should choose their papers by diversifying the range of approaches used to solve the problem. A good guideline could be to choose a paper from a decade or two ago, and a couple of more recent papers. You need to experimentally evaluate approaches in a simulation of a problem, in a range of scenarios, and analyse the pros and cons of each approach. 
%\end{description}

\section{Introduction}
\label{sec:introduction}
%In this section, you should introduce your work: what are the motivations behind this work? What is the relevant problem that you are investigating? Why is it relevant? 
%Briefly, introduce the background information required to understand the problem and the concepts that you will develop. 
%
%This section should also contain the link to the recording of your presentation (college OneDrive link – please make sure sharing permissions are such that everyone with tcd email can access it)
%
% TODO: Motivations behind this work
The Autonomous Vehicle (AV) industry is one of the most active and promising area in the recent years with many ongoing research to make it fully autonomous \cite{talavera2021autonomous}. The International Society of Automotive Engineers (SAE) proposed a six-degree autonomy scale with level 0 as no automation and level 5 as full automation without any human interventions \cite{stoma2021future}. The current level of autonomy scale achieved today is between level 2 and 3 requiring some assistance from the driver and limited to ideal conditions.

% TODO: What is the relevant problem that you are investigating
Car parking is challenging and disliked by most drivers due to time required searching for spaces, risk of scratching vehicle, safety of pedestrian, etc \cite{baburaj2021smart}\cite{bosch_global_2022}. It requires the driver to estimate the space required for the car and manoeuvre it into available space by controlling the steering angle and accelerator. While in fact the traffic report statistics survery results found in 12 million traffic accidents, there are about 10,000 traffic accidents occurring in the parking lots with many more number of accidents not reported \cite{wang2014automatic}.

% TODO: Why is it relevant
Most vehicles are parked $95\%$ throughout the lifetime \cite{choi2019self}. Parking is required by every driver and applying AV will greatly improve the quality of life and ease of drivers. It increases driving safety by utilising various sensors to understand the environment surrounding vehicle and park at the designated space avoid collisions due to lack of human experiences \cite{wang2014automatic}. Other potential application for autonomous car parking is saving spaces in car parking lots by parking in an optimal grid resulting in a much efficient parking lot capable of more car \cite{choi2019self}.

% TODO: Introduce background information required to understand the problem and concepts that you will developed
The highway-env github repository\footnote{https://github.com/eleurent/highway-env} contains a collection of autonomous driving and tactical decision-making tasks environment \cite{highway-env}. The parking environment\footnote{id of "parking-v0"} is selected for the project which is a goal-conditioned continuous control task in a given space with a vehicle aiming to reach the destination point. The parking environment uses a grid like layout similar to actual car parks. the vehicle requires input of the steering angle and accelerator. Policy based algorithm is commonly used for autonomous vehicles instead of value based algorithm due its large action decision requiring huge memory \cite{takehara2021autonomous}.
The selected algorithms to be investigated in the project are Soft-Actor-Critic (SAC) from the reinforcement learning family, behaviour cloning from the imitation family and evolutionary algorithm from the genetic population-based family.
% TODO: Include link to recording of presentation

\section{Related Work}
% In this section you will discuss possible approaches to solve the problem you are addressing, 
% justifying your choice of the 3 you have selected to evaluate. 
% Also, briefly introduce the approaches you are evaluating with a specific emphasis on differences and similarities to the proposed approach(es).
The autonomous vehicle control system consists of mission trajectory planning system generating path for vehicle to reach target, navigation system detecting environment and either moving or fixed target and trajectory tracking controller to navigate the vehicle accordingly \cite{lin2018path}. Path planning problem is the main subject of most autonomous vehicle studies. Prior to path planning, the algorithm needs to know the state of the vehicle such as its position and direction. GPS is commonly used to deduce the state by using signals Line Of Sight (LOS) from positioning satellite but not possible in the context of indoor parking lots \cite{correa2017autonomous} and high-rise buildings \cite{saksena2019towards}. Instead other data such as vision-based sensors and LiDAR sensors are used to monitor the state of vehicle \cite{chan2021review}. The works related to autonomous car parking uses different inputs and types algorithm families, this section mainly focuses on selected algorithms mentioned in Section \ref{sec:introduction}.

% TODO: Soft-Actor-Critic (SAC) related works
Deep Learning (DL) combined with Reinforcement Learning (RL) enables us to build machine learning models which does not require labelled data to learn about a given task. Reinforcement Learning enables an artificial agent to learn optimal strategy from the environment through trial and error \cite{yu2010introduction}. RL formalizes the premise that rewarding or penalizing an agent for its action increases the likelihood that the agent will repeat or avoid respective behaviour in the future.

Deep Reinforcement Learning (DRL) is useful when the agent needs to make a decision based on more than one contributing element or high-dimensional input data \cite{arulkumaran2017deep}.  Autonomous driving systems constitute of multiple tasks where classical supervised learning methods are no more applicable due its complexity and absence of labelled data.  Use of RL or Deep RL applied to autonomous driving or the self driving domain is an emergent field \cite{sainath2021application}.

The two types of deep reinforcement learning algorithm widely used these days is Policy Optimization and Deep Q Learning. Deep Q learning algorithms try to learn the optimal Q function Q(s,a) with a function approximator Q\_theta(s,a). It generally uses an objective function based on Bellman equation. To the contrary, Policy optimization algorithms try to directly learn the policy pi\_theta(a|s) by optimizing parameters theta by using gradient ascent on performance objective J(pi\_theta). 

The primary advantage of policy optimization approaches is that they are principled, in that we explicitly optimize for what we desire. This makes them more stable and dependable. Q-learning approaches, on the other hand, only indirectly improve agent performance by training Q theta to meet a self-consistency equation. Since there are so many failure scenarios in this type of learning, it is less stable. When they do work, however, Q-learning approaches have the benefit of being far more sample efficient than policy optimization techniques since they can reuse data more efficiently \cite{yang2020nature}. 

There are a number of algorithms that exist on this spectrum and are capable of carefully balancing the strengths and limitations of each side. Soft actor critic (SAC) \cite{sloss20202019} is a variation that stabilizes learning by using stochastic policies, entropy regularization, and a few more methods. After reaching the steady-state phase, the maximum entropy framework in SAC may diminish the efﬁciency of learning outcomes \cite{simon2013evolutionary}.  Highsight experience replay (HER) is a sample-efficient replay approach for off-policy DRL algorithms that allows the agent to learn from both failures and successes, comparable to humans. HER gives the agent a supplemental reward based on the idea of aim, which increases the efficiency of the learning outcomes even if the objective is not met \cite{simon2013evolutionary}.

% TODO: Behaviour Cloning related works
Behaviour Cloning (BC) is from the Imitation learning family which uses observations from the expert to learn \cite{torabi2018behavioral}. In paper \cite{saksena2019towards} trains a Recurrent Convolutional Network for autonomous driving in lane changing context by using BC to learn from human experts driving vehicles. The paper noted the ease of changing architecture when using BC technique and found that spatio-temporal properties worked well in dynamic environments. The huge advantage of BC algorithm is its end-to-end trainable architecture which uses supervised learning to learn policies from an expert by inferring expert's actions \cite{codevilla2019exploring}\cite{farag2018behavior}\cite{ly2020learning}\cite{saksena2019towards}. However the BC approach suffers from several limitations such as over-fitting and generalisation issue \cite{codevilla2019exploring}. BC algorithm is a simple yet effective approach in the project and it can be used to learn the policies created from other algorithms selected in the project.

% TODO: Evolutionary algorithm related works
Evolutionary algorithms are algorithms with ability to evolve in order to learn a task. EA’s are based on the concept of survival of the fittest principal of the modern genetic \cite{yu2010introduction}.  Genetic algorithm is a population based metaheuristic algorithm belonging to the family of evolutionary algorithms \cite{yu2010introduction}. Genetic algorithm is inspired by Darwin’s theory of evolution. Genetic algorithm is used to solve complex real world problems, one of which is autonomous cars modelling. In the paper \cite{arulkumaran2017deep} implements the genetic algorithm with the combination of artificial neural network which is also referred as Neuro-evolution to model vehicle dynamics and train autonomous cars. The paper states that evolutionary algorithms evolve via trial and error method over the iterations and hence provide advantages over the data dependent legacy approaches. It is also observed that the use of neural network with genetic algorithm can reduces the initial phase of generalising features and works without any prerequisite data for model training \cite{arulkumaran2017deep}.  One of the notable advantages of genetic algorithm is parallelism. The genomes in the population acts as independent agents, the algorithm is able to explore the search space in all the direction at a time \cite{kiran2021deep}. Although, both genetic algorithm and neural networks largely depend on the choice of parameters such as population size, cross-over and mutation rate, number of network layers. Hence wrong choices can lead to a situation where algorithm may not converge. 

\section{Problem Definition and Algorithm}
Problem definition and algorithm
%This section formalises the problem you are addressing and the models used to solve it. This section should provide a technical discussion of the chosen/implemented algorithms. A pseudocode description of the algorithm(s) can also be beneficial to a clear explanation. It is also possible to provide one example that clarifies the way an algorithm works. It is important to highlight in this section the possible parameters involved in the model and their impact, as well as all the implementation choices that can impact the algorithm.

\subsection{Soft-Actor-Critic Algorithm}
Deep Reinforcement Learning’s goal is to learn a new environment in a short amount of time and then generalize to other situations. In non-simulation contexts, the conditions may go unnoticed throughout training. Sample efficiency is a problem for some of the most effective RL algorithms in recent years, such as trust region policy optimization (TRPO), proximal policy optimization (PPO), and asynchronous actor-critic agents (A3C) \cite{sloss20202019}.

This is because these strategies allow for on-policy learning and necessitate new samples following each policy modification. Using experience replay buffers, Q-learning based off-policy algorithms like DDPG learn efficiently from past samples. However, such approaches are hyper-parameter sensitive and require a great deal of adjustment to converge. SAC is an alternate strategy for accelerating convergence.

HER is a sample-efficient replay method that improves off-policy DRL algorithm performance by allowing the agent to learn from both successes and failures. HER is applied to SAC and propose to help SAC learn more effectively. 


\subsection{Behaviour Cloning Algorithm}
Behaviour Cloning (BC) generates trajectory by learning policy from direct mapping of states to actions without recovering reward function of the expert \cite{osa2018algorithmic}.

\begin{equation}
\tau^d = \pi(s)
\label{eq:bc_general_goal}
\end{equation}

\begin{equation}
\upsilon_t = \pi(x_t)
\label{eq:bc_goal}
\end{equation}
Equation (\ref{eq:bc_general_goal}) is the general aim of BC algorithm to generate trajectory of agent by learning policy for a given state. The goal of BC for the project car parking in an action-state space can be formulated as equation (\ref{eq:bc_goal}) to generate control input $\upsilon_t$ using policy $\pi$ given the current state $x_t$. The driving task for BC can be treated as a supervised learning problem \cite{ly2020learning}\cite{osa2018algorithmic}\cite{saksena2019towards}. 

The abstract of BC algorithm described in \cite{osa2018algorithmic} requires a dataset of trajectories from the expert $\mathcal{D}$, policy representation $\pi_\theta$ and objective function $\mathcal{L}$. The result is an optimised policy parameters $\theta$ trained using the dataset $\mathcal{D}$ by optimising the objective function $\mathcal{L}$.
%
\begin{equation}
\mathcal{D} = \{ (\upsilon_i^*, s_i^*) \}^M_{i=1} = d^{\pi^*}
\label{eq:bc_dataset}
\end{equation}
Equation \ref{eq:bc_dataset} describes the dataset represented by a set of control input ($\upsilon_i$) and state ($s_i$) pair from the trajectories of the expert. The policy of the expert is used as the ground truth reward and assumed to be near the optimal policy $\pi^*$ \cite{lecture6789wen}.
\begin{equation}
\ell(x^L,x^{demo}) = (x^L - x^{demo})^T(x^L-x^{demo})
\label{eq:bc_l2}
\end{equation}
The quadratic loss function also known as $\ell_2$-loss or least squares (LS) in equation (\ref{eq:bc_l2}) is commonly used as an objective function and most optimiser libraries include additional regularisation term. The $x$ terms are vectors and loss calculates the action from agent's policy $x^L$ against expert's action $x^{demo}$. Other loss functions can be used as the object function such as $\ell_1$-loss, cross entropy loss, hinge loss, etc.

\begin{algorithm}
  \caption{Behaviour Cloning algorithm}
  \label{alg:bc_abstract}
  \begin{algorithmic}[1]
    \STATE expert\_trajectory $\mathcal{D}$ $\leftarrow$ sample from expert's policy from environment \label{op0}
    \STATE agent\_policy $\pi_\theta$ $\leftarrow$ initialise a random policy parameters $\theta$ \label{op1}
    \STATE train agent\_policy $\theta$ dataset expert\_trajectory $\mathcal{D}$  \label{op2}
    \STATE save optimised agent\_policy $\theta$ \label{op4}
  \end{algorithmic}
\end{algorithm}
The algorithm (\ref{alg:bc_abstract}) is a high-level pseudo-code of the behaviour cloning approach. Line \emph{3} uses supervised learning algorithms with hyper-parameters such as objective function $\mathcal{L}$, optimiser, network architecture, etc.

\subsection{Evolutionary Algorithm}
EAs are a family of algorithms which are inspired by biological process of natural selection and Darwin’s evolution theory. Neuro-evolutionary algorithms are the combination of neural network and genetic algorithm \cite{arulkumaran2017deep}. Each genome in this algorithm is a neural network. Weights and biases of the neural network are initialized randomly. Each takes the observation from environment as an input and return required actions. Based on these actions fitness score is calculated and returned by the network. The goal of neural network is to optimize the fitness score. Genetic algorithm works based on five stages: Population, Fitness evaluation, Selection, Crossover, Mutation \cite{russell2016artificial}\cite{haarnoja2018soft}.  

\textbf{Population}: Population is set of solution candidates to the problem we are dealing with. These individual candidates are also referred as genomes or chromosomes. Here each genome is a neural network whose weights and biases are randomly initialized. 

\textbf{Fitness evaluation}: Fitness function calculates a fitness score for each individual or genome which is the measure of difference between desired and current state. Each neural network returns a fitness score which is used in selection to generate future individuals.

\textbf{Selection}: Selection is the process of selecting neural networks based on their fitness score as parents for future generations. Selection process can be done in various ways. One of the most common approaches is ranked selection where individuals are order in descending order of their fitness score and those with higher fitness score are selected as parents.

\textbf{Crossover}: Crossover is the process where weights and biases of two neural networks are exchanged with each other. This process is also referred to biological mating where some genes from two individuals are pooled together to obtain new generation. Percentage of genes to be exchanged depends on crossover rate. New individuals which are referred as offspring are created until the crossover point is reached. These new individuals are further added to the population.

\textbf{Mutation}: Mutation is the process where some of the weights and biases of the neural network are flipped based on low probability. Mutation ensures that the population has diversity in population and further increases the search space for the algorithm.

\begin{algorithm}
  \caption{Genetic Algorithm pseudocode \cite{zouita2019improving}}
  \label{alg:ga_pseudo}
  \begin{algorithmic}[1]
    \STATE Random population initialization  \label{op0}
    \STATE Fitness evaluation of the population individuals \label{op1}
    \STATE Repeat \label{op2}
    \bindent
    \STATE Select neural networks with high fitness score as parents 
    \STATE Generate new individuals by crossover process
    \STATE Perform mutation on the newly created individuals
    \STATE Fitness evaluation of the newly created individuals
    \STATE Replace old worst individuals with new best individuals
    \eindent
    \STATE Stop until termination criteria     
  \end{algorithmic}
\end{algorithm}

%\subsection{Subsection Title}

\section{Experimental Results}
Experimental result
%This section should provide the details of the evaluation. Specifically:
%\begin{itemize}
%\item Methodology: describe the evaluation criteria, the data used during the evaluation, and the methodology followed to perform the evaluation. 
%\item Results: present the results of the experimental evaluation. Graphical data and tables are two common ways to present the results. Also, a comparison with a baseline should be provided.
%\item Discussion: discuss the implication of the results of the proposed algorithms/models. What are the weakness/strengths of the method(s) compared with the other methods/baseline?
%\end{itemize}

\section{Conclusions}
Conclusion
%Provide a final discussion of the main results and conclusions of the report. Comment on the lesson learnt and possible improvements.
%
%
%A standard and well formatted bibliography of papers cited in the report. For example:

\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file
%\begin{thebibliography}{6}
%%
%
%\bibitem {smit:wat}
%Smith, T.F., Waterman, M.S.: Identification of common molecular subsequences.
%J. Mol. Biol. 147, 195?197 (1981). \url{doi:10.1016/0022-2836(81)90087-5}
%
%\bibitem {may:ehr:stein}
%May, P., Ehrlich, H.-C., Steinke, T.: ZIB structure prediction pipeline:
%composing a complex biological workflow through web services.
%In: Nagel, W.E., Walter, W.V., Lehner, W. (eds.) Euro-Par 2006.
%LNCS, vol. 4128, pp. 1148?1158. Springer, Heidelberg (2006).
%\url{doi:10.1007/11823285_121}
%
%\bibitem {fost:kes}
%Foster, I., Kesselman, C.: The Grid: Blueprint for a New Computing Infrastructure.
%Morgan Kaufmann, San Francisco (1999)
%
%\bibitem {czaj:fitz}
%Czajkowski, K., Fitzgerald, S., Foster, I., Kesselman, C.: Grid information services
%for distributed resource sharing. In: 10th IEEE International Symposium
%on High Performance Distributed Computing, pp. 181?184. IEEE Press, New York (2001).
%\url{doi: 10.1109/HPDC.2001.945188}
%
%\bibitem {fo:kes:nic:tue}
%Foster, I., Kesselman, C., Nick, J., Tuecke, S.: The physiology of the grid: an open grid services architecture for distributed systems integration. Technical report, Global Grid
%Forum (2002)
%
%\bibitem {onlyurl}
%National Center for Biotechnology Information. \url{http://www.ncbi.nlm.nih.gov}
%\end{thebibliography}
\end{document}
